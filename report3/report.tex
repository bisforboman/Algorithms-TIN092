\documentclass[a4paper,11pt]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{rotating}
\usepackage{listings}
\usepackage{color}
\usepackage{listings}
\usepackage{amsmath}

\title{Algorithms, assignment - part 3 by Group 26}
\author{Arash Rouhani (rarash@student.chalmers.se) - 901117-1213\\
        Jakob Boman (bisforboman@gmail.com) - 901014-1357}

\begin{document}

\maketitle

\section{Dynamic Programming Strategy}
The complete solution considers all nodes. We consider a partial solution
of the problem to evaluate the expected latency for a subset of the nodes,
from a chosen starting point. Note that we consider that we have not
traveled any distance when solving for the partial problem.

So our parameters becomes $V$ and $i$, the set of nodes to visit and the
starting node. Our function returns an expected latency.

$$ EL(\{\}, u) = 0 $$
\begin{equation*}
\begin{aligned}
& EL(V, u) =
& & \underset{v \in V}{\text{minimize}}
& & & EL(V\setminus \{v\}, v) + c_{u,v} * d(universe\setminus V) \\
\end{aligned}
\end{equation*}

The base case is trivial, so lets focus on the inductive step.
The node $u$ (the one we traverse to) will cost
$(distance\_traveled+c_{v,u})*d(u)$ in the end, but the actual
$distance\_traveled$ is unknown to us since we say that we assume
we "start" from $u$ and haven't traversed anything.

So instead of dealing with what cost the node $u$ will cost, we look at the
edge $u \to v$. Examine how the traversed distance $c_{v,u}$ will affect the
resulting expected latency, Instead we calculate the latency the taken edge will cause
for all remaining probabilities.
The \emph{sum} for the probabilities of the remaining nodes are
$d(universe\setminus V)$.

\subsection{execution of instance 1}

\begin{lstlisting}
./Part3 < inst1.txt
Min. Expected Latency: 29.3273
Path: 1 4 2 3 5
\end{lstlisting}

\subsection{Complexity}

Basically each "call" to EL does a loop of $O(r)$ where
$r$ is the number of remaining nodes. In the loop
we calculate the subproblem with one node less.
Therefore the complexity is $O(n)*O(n-1)*O(n-2)*...*O(1) = O(n!)$.



\section{Improving Efficiency}

There are $(n-1)!$ different paths, however, when looking at our
recursive definition of a subproblem, we see there are only
$2^n*n$ different parameter-combinations, that is clearly less. This
is intuitive, only needing to care for remaining
nodes is a lot less burden than needing to know taken path.
It is therefore clear that our $EL$ should be memoized, since there
will be many paths "passing the same arguments" to $EL$.

We memorize two values, firstly, the return value of EL, secondly,
we memorize the chosen edge, so we can reconstruct the path.
Note that both these values are stored in tables of memory $O(2^n*n)$.

We have run our program on $instance 1$ twice, with memoization
it did 29 calls that were firsttime unique parameters.
65 without memoization.

We know there are a $O(2^n*n)$ parameter-combinations for this method,
and each calculation does $O(n)$ so resulting in $O(2^n*n^2)$
time complexity.


\section{Pseudocode}
We have two lexicas $(NodeSet, Node) \to double$ and $(NodeSet, Node) \to Node$
for $mem$ and $savedEdge$ respectively.

Just as in the problem statement, $d$ is the array of probabilities
and $c$ is the edge-cost matrix.

In the pseudocode, $lat_v$ is the value we minimize over,
it is mathematically defined as in the equation in chapter 1.

\begin{lstlisting}[mathescape]

double solve(NodeSet remaining, Node current, double probsRemaining){

    If mem[remaining][current] is written then
        return mem[remaining][current]

    If nodeset $remaining$ is empty then
        return 0

    ForEach Node $v$ in remaining do
        $lat_v$ = solve($remaining \setminus \{v\}$, u, probsRemaining - d[u])
                       + c[current][u]*probsRemaining

    mem[remaining][current] = smallest element in $lat_v$
    savedEdge[remaining][current] = $v$ where $lat_v$ is the smallest

    return mem[remaining][current]
}

\end{lstlisting}

The answer is given by
solve($universe \setminus \{startNode\}$, startNode, 1-d[startNode])

\section{Complexity}
We memorize over all the $2^n*n$ parameter-combinations, so we
have $O(2^n*n)$ space complexity, obviously nothing can beat this
lower bound so we just say that's the correct upper bound.

As for time complexity, the part that isn't $O(1)$
in the pseudocode is the ForEach loop that is $O(n)$.
When we state $O(n)$ We "assume" that a recursive call to $solve$
is $O(1)$, that is $solve$ is memoized.
That assumption is ok if we count in the cost of memoizing all
the $O(2^n*n)$ function calls. And in a fully memoized
environment we said that solve takes $O(n)$, so memoizing
everything costs $O(2^n*n^2)$, and then we are done.
Note how we say that we memoize for all combinations,
so $O(2^n*n^2)$ is an upper bound.

\section{Performance}

\begin{center}
    \begin{tabular}{ | l | l | p{4cm} | p{5cm} |}
    \hline
    Filename & Time(s) & Property & Interpretation \\ \hline
    inst1 & 0.000 & n=5 & \\ \hline
    inst2 & 0.000 & n=10 & \\ \hline
    inst3 & 0.004 & n=12 & \\ \hline
    inst4 & 0.042 & n=15 & \\ \hline
    inst5 & 0.562 & n=18 & Runtime is noticeable. \\ \hline
    inst6 & 1.365 & n=19 & \\ \hline
    inst7 & 3.772 & n=20 & \\ \hline
    inst7\_large\_edgecosts & 3.784 & n=20 & Same as inst7. Edgecosts have no effect.
        This is obvious as the complexity is stated to only depend on $n$ \\ \hline
    \end{tabular}
\end{center}

Lets study the increased runtime for inst5..inst7.
We notice that it clearly isn't $O(n!)$ anymore, $n$ increasing with 1
is approximately a little more than twice the runtime.

If we would draw a cartesian graph, with $n$ and $edgecosts$ as the axises, plotting the runtime,
we would see that $edgecosts$ makes no difference, and that runtime grows for each
step we increase $n$. A step will increase the runtime by more than twice.

Finally we compare with the runtimes from lab 1:


\begin{center}
    \begin{tabular}{ | l | l | p{4cm} | p{5cm} |}
    \hline
    Filename & Time(s) & Property & Interpretation \\ \hline
    inst1 & 0.002 & n=5 \\ \hline
    inst2 & 0.594 & n=10 \\ \hline
    inst2\_weird\_edgecosts & 0.596 \\ \hline
    inst2\_weird\_probs & 0.595 & \\ \hline
    inst3 & 89.417 & n=12 \\ \hline
    \end{tabular}
\end{center}

The runtime improvement is clear.

\section{Appendix}
No external resources was used.

All code, test data and more is available at the project's \href{https://github.com/bisforboman/Algorithms-TIN092}{github page}.

A testrun of our program, we compile the program using g++.

\begin{lstlisting}
> g++ -O2 -o Part3 Part3.cpp
> ./Part3 < inst1.txt
Min. Expected Latency: 29.3273
Path: 1 4 2 3 5
Num calculations: 33

\end{lstlisting}

\end{document}
